{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fine-grained classification practice with Flower-17\n",
    "\"\"\"\n",
    "\n",
    "# Python Packages\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "# 3rd Party Packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "# User Packages\n",
    "from start.preprocessing import ImageToTensorPreprocessor, ResizePreprocessor\n",
    "from start.loader import ImageDataset\n",
    "from start.model import MiniVGGNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing label: person\n",
      "[INFO] processed 80 person images\n",
      "[INFO] processed 160 person images\n",
      "[INFO] processed 240 person images\n",
      "[INFO] processed 320 person images\n",
      "[INFO] processed 400 person images\n",
      "[INFO] processed 480 person images\n",
      "[INFO] processed 560 person images\n",
      "[INFO] processed 560 person images\n",
      "[INFO] processed 640 person images\n",
      "[INFO] processed 720 person images\n",
      "[INFO] processed 800 person images\n",
      "[INFO] processed 880 person images\n",
      "[INFO] processed 960 person images\n",
      "[INFO] processed 1040 person images\n",
      "[INFO] Processing label: soldier\n",
      "[INFO] processed 80 soldier images\n",
      "[INFO] processed 160 soldier images\n",
      "[INFO] processed 240 soldier images\n",
      "[INFO] processed 320 soldier images\n",
      "[INFO] processed 400 soldier images\n",
      "[INFO] processed 480 soldier images\n",
      "[INFO] processed 560 soldier images\n",
      "[INFO] processed 640 soldier images\n",
      "[INFO] processed 720 soldier images\n",
      "[INFO] processed 800 soldier images\n",
      "[INFO] processed 880 soldier images\n",
      "[INFO] processed 960 soldier images\n",
      "[INFO] processed 1040 soldier images\n",
      "data shape: (2177, 224, 224, 3)\n",
      "labels shape: (2177,)\n",
      "classes: {'soldier', 'person'}\n"
     ]
    }
   ],
   "source": [
    "# Load Flowers-17 dataset\n",
    "dataset = ImageDataset(\n",
    "    preprocessors=[\n",
    "        ResizePreprocessor(224, 224, aspect_preserving=True),\n",
    "        ImageToTensorPreprocessor()\n",
    "    ]\n",
    ")\n",
    "\"\"\"\n",
    "(data, labels) = dataset.load(\n",
    "    dataset_path=r'/home/share/dataset/flowers17',\n",
    "    verbosity=80\n",
    ")\n",
    "\"\"\"\n",
    "(data, labels) = dataset.load(\n",
    "    dataset_path=r'T:\\temp\\simeon\\dataset\\custom',\n",
    "    verbosity=80\n",
    ")\n",
    "\n",
    "classes = set(labels)\n",
    "\n",
    "print('data shape: {}'.format(data.shape))\n",
    "print('labels shape: {}'.format(labels.shape))\n",
    "print('classes: {}'.format(classes))\n",
    "\n",
    "\n",
    "# Normalize data\n",
    "data = data.astype(np.float) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n",
      "Training on fold 0/10\n",
      "[INFO] training network...\n",
      "Epoch 1/35\n"
     ]
    }
   ],
   "source": [
    "# Setup data splits\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import pycm\n",
    "\n",
    "N_EPOCHS = 35\n",
    "BATCH_SIZE = 32\n",
    "n_classes = len(classes)\n",
    "kfold_splits = 10\n",
    "\n",
    "# Instantiate the cross validator\n",
    "skf = StratifiedKFold(n_splits=kfold_splits, shuffle=True)\n",
    "\n",
    "(trainX, testX, trainY, testY) = train_test_split(\n",
    "    data, labels,\n",
    "    test_size=0.2,\n",
    "    random_state=int(time.time()),\n",
    "    stratify=list(labels)\n",
    ")\n",
    "\n",
    "# Convert output to either binarized one-hot vectors for categorical or 0/1 for binary\n",
    "if n_classes > 2:\n",
    "    lb = LabelBinarizer()\n",
    "    testY = lb.fit_transform(testY)\n",
    "else:\n",
    "    le = LabelEncoder()\n",
    "    testY = le.fit_transform(testY)\n",
    "    \n",
    "# Data augmentation\n",
    "augmenter = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "def create_model(n_classes, alpha=1.4):\n",
    "    # Initialize the optimizer and model\n",
    "    print('[INFO] compiling model...')\n",
    "    opt = SGD(lr=0.05)\n",
    "    properties = {\n",
    "        'width':    64,\n",
    "        'height':   64,\n",
    "        'channels': 3,\n",
    "        'classes':  len(classes)\n",
    "    }\n",
    "    #model = MiniVGGNet.build(properties)\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense\n",
    "    model = Sequential()\n",
    "    model.add(MobileNetV2(\n",
    "        input_shape=(224, 224, 3),\n",
    "        alpha=alpha, \n",
    "        depth_multiplier=1, \n",
    "        include_top=False, \n",
    "        weights='imagenet', \n",
    "        input_tensor=None, \n",
    "        pooling='avg'\n",
    "    ))\n",
    "\n",
    "    if n_classes > 2:\n",
    "        model.add(Dense(\n",
    "            units=n_classes,\n",
    "            activation='softmax',\n",
    "            use_bias=True,\n",
    "            kernel_initializer='glorot_uniform',\n",
    "            bias_initializer='glorot_uniform',\n",
    "            kernel_regularizer=None,\n",
    "            bias_regularizer=None,\n",
    "            activity_regularizer=None,\n",
    "            kernel_constraint=None,\n",
    "            bias_constraint=None\n",
    "        ))\n",
    "        model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer=opt,\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "    # Hot dog or not hot dog\n",
    "    else:\n",
    "        model.add(Dense(\n",
    "            units=1,\n",
    "            activation='sigmoid',\n",
    "            use_bias=True,\n",
    "            kernel_initializer='glorot_uniform',\n",
    "            bias_initializer='glorot_uniform',\n",
    "            kernel_regularizer=None,\n",
    "            bias_regularizer=None,\n",
    "            activity_regularizer=None,\n",
    "            kernel_constraint=None,\n",
    "            bias_constraint=None\n",
    "        ))\n",
    "        model.compile(\n",
    "            loss='binary_crossentropy',\n",
    "            optimizer=opt,\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "model = create_model(n_classes=n_classes)\n",
    "\n",
    "for index, (train_indices, val_indices) in enumerate(skf.split(trainX, trainY)):\n",
    "    print('Training on fold {}/{}'.format(index, kfold_splits))\n",
    "    # Partition into train and test splits\n",
    "    \"\"\"\n",
    "    (valX, testX, valY, testY) = train_test_split(\n",
    "        testX, testY,\n",
    "        test_size=0.4,\n",
    "        random_state=int(time.time()),\n",
    "        stratify=list(testY)\n",
    "    )\n",
    "    \"\"\"\n",
    "    trainSplitX, valSplitX = trainX[train_indices], trainX[val_indices]\n",
    "    trainSplitY, valSplitY = trainY[train_indices], trainY[val_indices]\n",
    "    \n",
    "    # Convert output to either binarized one-hot vectors for categorical or 0/1 for binary\n",
    "    if n_classes > 2:\n",
    "        trainSplitY = lb.fit_transform(trainSplitY)\n",
    "        valSplitY = lb.fit_transform(valSplitY)\n",
    "    else:\n",
    "        trainSplitY = le.fit_transform(trainSplitY)\n",
    "        valSplitY = le.fit_transform(valSplitY)\n",
    "\n",
    "    # Initialize TensorBoard\n",
    "    tb_callback = TensorBoard(\n",
    "        log_dir='./logs/splitindex{}'.format(index), \n",
    "        histogram_freq=1, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        write_graph=False, \n",
    "        write_grads=False, \n",
    "        write_images=False, \n",
    "        embeddings_freq=0,\n",
    "        embeddings_layer_names=None, \n",
    "        embeddings_metadata=None, \n",
    "        embeddings_data=None\n",
    "    )\n",
    "\n",
    "\n",
    "    # Train the network\n",
    "    print('[INFO] training network split {}...'.format(index))\n",
    "    history = model.fit_generator(\n",
    "        augmenter.flow(trainSplitX, trainSplitY, \n",
    "                       batch_size=BATCH_SIZE),\n",
    "        validation_data=(valSplitX, valSplitY),\n",
    "        steps_per_epoch=len(trainX) // BATCH_SIZE,\n",
    "        epochs=N_EPOCHS,\n",
    "        callbacks=[tb_callback],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate the network\n",
    "    print('[INFO] evaluating network split {}...'.format(index))\n",
    "    predictions = model.predict(testX, batch_size=BATCH_SIZE)\n",
    "    if not n_classes > 2:\n",
    "        threshold = 0.5\n",
    "        predictions[predictions>threshold] = 1\n",
    "        predictions[predictions<=threshold] = 0\n",
    "        predictions = predictions.astype(np.int)\n",
    "\n",
    "    if n_classes > 2:\n",
    "        cm = pycm.ConfusionMatrix(\n",
    "            actual_vector=lb.inverse_transform(testY),\n",
    "            predict_vector=lb.inverse_transform(predictions)\n",
    "        )\n",
    "    else:\n",
    "        cm = pycm.ConfusionMatrix(\n",
    "            actual_vector=le.inverse_transform(testY),\n",
    "            predict_vector=le.inverse_transform(predictions)\n",
    "        )\n",
    "    cm.save_html(r'T:\\temp\\simeon\\dataset\\confusion_matrix_splitindex{}'.format(index))\n",
    "\n",
    "    # Plot the training loss and accuracy\n",
    "    plt.style.use('ggplot')\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(0, N_EPOCHS), history.history['loss'], label='train_loss')\n",
    "    plt.plot(np.arange(0, N_EPOCHS), history.history['val_loss'], label='val_loss')\n",
    "    plt.plot(np.arange(0, N_EPOCHS), history.history['acc'], label='train_acc')\n",
    "    plt.plot(np.arange(0, N_EPOCHS), history.history['val_acc'], label='val_acc')\n",
    "    plt.title('Training Loss and Accuracy')\n",
    "    plt.xlabel('Epoch #')\n",
    "    plt.ylabel('Loss/Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(r'T:\\temp\\simeon\\dataset\\training_splitindex{}.jpg'.format(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot images that were soldiers classified as people\n",
    "print('classes: {}'.format(classes))\n",
    "print('testY shape: {}'.format(testY.shape))\n",
    "print('predictions shape: {}'.format(predictions.shape))\n",
    "soldiers_classified_people = testX[np.logical_and(testY == 1, np.squeeze(predictions) == 0)]\n",
    "print('soldiers_classified_people shape: {}'.format(soldiers_classified_people.shape))\n",
    "# Draw figure\n",
    "if soldiers_classified_people.shape[0] > 0:\n",
    "    IMAGES_PER_ROW = 4\n",
    "    rows = (soldiers_classified_people.shape[0] // IMAGES_PER_ROW) + 1\n",
    "    f = plt.figure(\n",
    "        num=1, \n",
    "        figsize=(14, int(14*rows/float(IMAGES_PER_ROW))), \n",
    "        dpi=80, \n",
    "        facecolor='w', \n",
    "        edgecolor='k'\n",
    "    )\n",
    "    print('{} x {} plot'.format(rows, IMAGES_PER_ROW))\n",
    "    for subidx in range(soldiers_classified_people.shape[0]):\n",
    "        # subplot is 1-indexed\n",
    "        plt.subplot(rows, IMAGES_PER_ROW, subidx+1)\n",
    "        f.gca().grid(False)\n",
    "        plt.imshow(soldiers_classified_people[subidx][..., ::-1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot images that were soldiers classified as people\n",
    "print('classes: {}'.format(classes))\n",
    "print('testY shape: {}'.format(testY.shape))\n",
    "print('predictions shape: {}'.format(predictions.shape))\n",
    "people_classified_soldiers = testX[np.logical_and(testY == 0, np.squeeze(predictions) == 1)]\n",
    "print('people_classified_soldiers shape: {}'.format(people_classified_soldiers.shape))\n",
    "# Draw figure\n",
    "if people_classified_soldiers.shape[0] > 0:\n",
    "    IMAGES_PER_ROW = 4\n",
    "    rows = (people_classified_soldiers.shape[0] // IMAGES_PER_ROW) + 1\n",
    "    f = plt.figure(\n",
    "        num=2, \n",
    "        figsize=(14, int(14*rows/float(IMAGES_PER_ROW))), \n",
    "        dpi=80, \n",
    "        facecolor='w', \n",
    "        edgecolor='k'\n",
    "    )\n",
    "    print('{} x {} plot'.format(rows, IMAGES_PER_ROW))\n",
    "    for subidx in range(people_classified_soldiers.shape[0]):\n",
    "        # subplot is 1-indexed\n",
    "        plt.subplot(rows, IMAGES_PER_ROW, subidx+1)\n",
    "        f.gca().grid(False)\n",
    "        plt.imshow(people_classified_soldiers[subidx][..., ::-1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
