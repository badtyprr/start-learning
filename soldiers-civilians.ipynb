{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 1.12.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fine-grained classification practice with Flower-17\n",
    "\"\"\"\n",
    "\n",
    "# Python Packages\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "# 3rd Party Packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "import cv2\n",
    "# User Packages\n",
    "from start.preprocessing import ImageToTensorPreprocessor, ResizePreprocessor, ColorSpacePreprocessor\n",
    "from start.loader import ImageCachedDataset\n",
    "from start.model import MiniVGGNet\n",
    "\n",
    "print('Tensorflow version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing label: person\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-432635faa0d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \"\"\"\n\u001b[1;32m     17\u001b[0m (data, labels) = dataset.load(\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mverbosity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\notebooks\\Simeon\\start-learning\\start\\loader\\base.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, verbosity)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marrays\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtest\u001b[0m \u001b[0msets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbosity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\notebooks\\Simeon\\start-learning\\start\\loader\\image.py\u001b[0m in \u001b[0;36m_cached_load\u001b[0;34m(self, verbosity)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marrays\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtest\u001b[0m \u001b[0msets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \"\"\"\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandlers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbosity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\notebooks\\Simeon\\start-learning\\start\\loader\\base.py\u001b[0m in \u001b[0;36m_directory_handler\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvariable\u001b[0m \u001b[0mtype\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \"\"\"\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_directory_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\notebooks\\Simeon\\start-learning\\start\\loader\\image.py\u001b[0m in \u001b[0;36m_cached_directory_handler\u001b[0;34m(self, verbosity)\u001b[0m\n\u001b[1;32m     42\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                         image = self._cached_retrieve(\n\u001b[0;32m---> 44\u001b[0;31m                             \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m                         )\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\notebooks\\Simeon\\start-learning\\start\\loader\\image.py\u001b[0m in \u001b[0;36m_cached_retrieve\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# Check that the shape is at least rank 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;31m# Return the final preprocessed image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "# Load Flowers-17 dataset\n",
    "dataset = ImageCachedDataset(\n",
    "    preprocessors=[\n",
    "        ResizePreprocessor(224, 224, aspect_preserving=True),\n",
    "        ColorSpacePreprocessor(conversion=cv2.COLOR_BGR2GRAY),\n",
    "        ImageToTensorPreprocessor()\n",
    "    ],\n",
    "    dataset_path=r'T:\\temp\\simeon\\dataset\\custom'\n",
    ")\n",
    "dataset._ignored_labels.append('firearm')\n",
    "\"\"\"\n",
    "(data, labels) = dataset.load(\n",
    "    dataset_path=r'/home/share/dataset/flowers17',\n",
    "    verbosity=80\n",
    ")\n",
    "\"\"\"\n",
    "(data, labels) = dataset.load(\n",
    "    verbosity=80\n",
    ")\n",
    "\n",
    "classes = set(labels)\n",
    "\n",
    "print('data shape: {}'.format(data.shape))\n",
    "print('labels shape: {}'.format(labels.shape))\n",
    "print('classes: {}'.format(classes))\n",
    "\n",
    "\n",
    "# Normalize data\n",
    "data = data.astype(np.float) / 255.0\n",
    "\n",
    "# Split into train and test sets\n",
    "(trainX, testX, trainY, testY) = train_test_split(\n",
    "    data, labels,\n",
    "    test_size=0.2,\n",
    "    random_state=int(time.time()),\n",
    "    stratify=list(labels)\n",
    ")\n",
    "\n",
    "# Free up the memory\n",
    "del data\n",
    "del labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup data splits\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import pycm\n",
    "\n",
    "N_EPOCHS = 35\n",
    "BATCH_SIZE = 32\n",
    "n_classes = len(classes)\n",
    "kfold_splits = 10 #empirical\n",
    "timestamp = time.time()\n",
    "\n",
    "# Instantiate the cross validator\n",
    "skf = StratifiedKFold(n_splits=kfold_splits, shuffle=True)\n",
    "\n",
    "# Convert output to either binarized one-hot vectors for categorical or 0/1 for binary\n",
    "if n_classes > 2:\n",
    "    lb = LabelBinarizer()\n",
    "    testY = lb.fit_transform(testY)\n",
    "else:\n",
    "    le = LabelEncoder()\n",
    "    testY = le.fit_transform(testY)\n",
    "    \n",
    "# Data augmentation\n",
    "augmenter = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize the optimizer and model\n",
    "print('[INFO] compiling model...')\n",
    "#model = create_model(n_classes=n_classes)\n",
    "from start.model import MobileNetV2\n",
    "model = MobileNetV2.build({\n",
    "    'width':    224,\n",
    "    'height':   224,\n",
    "    'channels': 1,\n",
    "    'weights': 'imagenet',\n",
    "    'classes': classes,\n",
    "    'dense_units': 242,\n",
    "    'dropout_rate': 0.43,\n",
    "    'regularization_strength': 0.0001\n",
    "})\n",
    "\n",
    "# Optimizer\n",
    "opt = SGD(lr=0.05)\n",
    "\n",
    "# Compile\n",
    "if n_classes > 2:\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=opt,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "else:\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=opt,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "print('[INFO] Finished compiling model...')\n",
    "            \n",
    "\n",
    "for index, (train_indices, val_indices) in enumerate(skf.split(trainX, trainY)):\n",
    "    print('Training on fold {}/{}'.format(index, kfold_splits))\n",
    "\n",
    "    #trainSplitX, valSplitX = trainX[train_indices], trainX[val_indices]\n",
    "    trainSplitY, valSplitY = trainY[train_indices], trainY[val_indices]\n",
    "    \n",
    "    # Convert output to either binarized one-hot vectors for categorical or 0/1 for binary\n",
    "    if n_classes > 2:\n",
    "        trainSplitY = lb.fit_transform(trainY[train_indices])\n",
    "        valSplitY = lb.fit_transform(trainY[val_indices])\n",
    "    else:\n",
    "        trainSplitY = le.fit_transform(trainY[train_indices])\n",
    "        valSplitY = le.fit_transform(trainY[val_indices])\n",
    "\n",
    "    # Initialize TensorBoard\n",
    "    \"\"\"\n",
    "    tb_callback = TensorBoard(\n",
    "        log_dir='./logs/splitindex{}'.format(index), \n",
    "        histogram_freq=1, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        write_graph=False, \n",
    "        write_grads=False, \n",
    "        write_images=False, \n",
    "        embeddings_freq=0,\n",
    "        embeddings_layer_names=None, \n",
    "        embeddings_metadata=None, \n",
    "        embeddings_data=None\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    # Train the network\n",
    "    print('[INFO] training network split {}...'.format(index))\n",
    "    callbacks = []\n",
    "    try:\n",
    "        if tb_callback is not None:\n",
    "            callbacks.append(tb_callback)\n",
    "    except NameError:\n",
    "        pass\n",
    "    history = model.fit_generator(\n",
    "        augmenter.flow(trainX[train_indices], trainSplitY, \n",
    "                       batch_size=BATCH_SIZE),\n",
    "        validation_data=(trainX[val_indices], valSplitY),\n",
    "        steps_per_epoch=len(trainX) // BATCH_SIZE,\n",
    "        epochs=N_EPOCHS,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate the network\n",
    "    print('[INFO] evaluating network split {}...'.format(index))\n",
    "    predictions = model.predict(testX, batch_size=BATCH_SIZE)\n",
    "    if not n_classes > 2:\n",
    "        threshold = 0.5\n",
    "        predictions_probability = predictions\n",
    "        predictions[predictions>threshold] = 1\n",
    "        predictions[predictions<=threshold] = 0\n",
    "        predictions = predictions.astype(np.int)\n",
    "        \n",
    "\n",
    "    if n_classes > 2:\n",
    "        cm = pycm.ConfusionMatrix(\n",
    "            actual_vector=lb.inverse_transform(testY),\n",
    "            predict_vector=lb.inverse_transform(predictions)\n",
    "        )\n",
    "    else:\n",
    "        cm = pycm.ConfusionMatrix(\n",
    "            actual_vector=le.inverse_transform(testY),\n",
    "            predict_vector=le.inverse_transform(predictions)\n",
    "        )\n",
    "    cm.save_html(r'T:\\temp\\simeon\\dataset\\{}_confusion_matrix_splitindex{}'.format(timestamp, index))\n",
    "\n",
    "    # Plot the training loss and accuracy\n",
    "    plt.style.use('ggplot')\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(0, N_EPOCHS), history.history['loss'], label='train_loss')\n",
    "    plt.plot(np.arange(0, N_EPOCHS), history.history['val_loss'], label='val_loss')\n",
    "    plt.plot(np.arange(0, N_EPOCHS), history.history['acc'], label='train_acc')\n",
    "    plt.plot(np.arange(0, N_EPOCHS), history.history['val_acc'], label='val_acc')\n",
    "    plt.title('Training Loss and Accuracy')\n",
    "    plt.xlabel('Epoch #')\n",
    "    plt.ylabel('Loss/Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(r'T:\\temp\\simeon\\dataset\\{}_training_splitindex{}.jpg'.format(timestamp, index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the network\n",
    "predictions = model.predict(testX, batch_size=BATCH_SIZE)\n",
    "if not n_classes > 2:\n",
    "    threshold = 0.5\n",
    "    predictions_probability = predictions.copy()\n",
    "    predictions[predictions>threshold] = 1\n",
    "    predictions[predictions<=threshold] = 0\n",
    "    predictions = predictions.astype(np.int)\n",
    "\n",
    "# Plot images that were soldiers classified as people\n",
    "print('classes: {}'.format(classes))\n",
    "print('testY shape: {}'.format(testY.shape))\n",
    "print('predictions shape: {}'.format(predictions.shape))\n",
    "soldiers_classified_people = testX[np.logical_and(testY == 1, np.squeeze(predictions) == 0)]\n",
    "try:\n",
    "    probability = predictions_probability[np.logical_and(testY == 1, np.squeeze(predictions) == 0)]\n",
    "except NameError:\n",
    "    pass\n",
    "print('soldiers_classified_people shape: {}'.format(soldiers_classified_people.shape))\n",
    "print('probability shape: {}'.format(probability.shape))\n",
    "print('probability: \\n{}'.format(probability))\n",
    "# Draw figure\n",
    "if soldiers_classified_people.shape[0] > 0:\n",
    "    IMAGES_PER_ROW = 4\n",
    "    rows = (soldiers_classified_people.shape[0] // IMAGES_PER_ROW) + 1\n",
    "    f = plt.figure(\n",
    "        num=1, \n",
    "        figsize=(14, int(14*rows/float(IMAGES_PER_ROW))), \n",
    "        dpi=80, \n",
    "        facecolor='w', \n",
    "        edgecolor='k'\n",
    "    )\n",
    "    print('{} x {} plot'.format(rows, IMAGES_PER_ROW))\n",
    "    for subidx in range(soldiers_classified_people.shape[0]):\n",
    "        # subplot is 1-indexed\n",
    "        plt.subplot(rows, IMAGES_PER_ROW, subidx+1)\n",
    "        f.gca().grid(False)\n",
    "        try:\n",
    "            plt.title('Soldier: {0:.2f}%'.format(np.asscalar(probability[subidx])*100.0))\n",
    "        except NameError:\n",
    "            pass\n",
    "        plt.imshow(soldiers_classified_people[subidx][..., ::-1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the network\n",
    "predictions = model.predict(testX, batch_size=BATCH_SIZE)\n",
    "if not n_classes > 2:\n",
    "    threshold = 0.5\n",
    "    predictions_probability = predictions.copy()\n",
    "    predictions[predictions>threshold] = 1\n",
    "    predictions[predictions<=threshold] = 0\n",
    "    predictions = predictions.astype(np.int)\n",
    "\n",
    "# Plot images that were people classified as soldiers\n",
    "print('classes: {}'.format(classes))\n",
    "print('testY shape: {}'.format(testY.shape))\n",
    "print('predictions shape: {}'.format(predictions.shape))\n",
    "people_classified_soldiers = testX[np.logical_and(testY == 0, np.squeeze(predictions) == 1)]\n",
    "try:\n",
    "    probability = predictions_probability[np.logical_and(testY == 0, np.squeeze(predictions) == 1)]\n",
    "except NameError:\n",
    "    pass\n",
    "print('people_classified_soldiers shape: {}'.format(people_classified_soldiers.shape))\n",
    "print('probability shape: {}'.format(probability.shape))\n",
    "print('probability: \\n{}'.format(probability))\n",
    "# Draw figure\n",
    "if people_classified_soldiers.shape[0] > 0:\n",
    "    IMAGES_PER_ROW = 4\n",
    "    rows = (people_classified_soldiers.shape[0] // IMAGES_PER_ROW) + 1\n",
    "    f = plt.figure(\n",
    "        num=2, \n",
    "        figsize=(14, int(14*rows/float(IMAGES_PER_ROW))), \n",
    "        dpi=80, \n",
    "        facecolor='w', \n",
    "        edgecolor='k'\n",
    "    )\n",
    "    print('{} x {} plot'.format(rows, IMAGES_PER_ROW))\n",
    "    for subidx in range(people_classified_soldiers.shape[0]):\n",
    "        # subplot is 1-indexed\n",
    "        plt.subplot(rows, IMAGES_PER_ROW, subidx+1)\n",
    "        f.gca().grid(False)\n",
    "        try:\n",
    "            plt.title('Person: {0:.2f}%'.format(np.asscalar(1.0-probability[subidx])*100.0))\n",
    "        except NameError:\n",
    "            pass\n",
    "        plt.imshow(people_classified_soldiers[subidx][..., ::-1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model!\n",
    "model.save_weights(r'T:\\temp\\simeon\\dataset\\{}_mnv2_soldier-person_StratifiedKFold10_weights.h5'.format(timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
