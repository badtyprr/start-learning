{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fine-grained classification practice with Flower-17\n",
    "\"\"\"\n",
    "\n",
    "# Python Packages\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "# 3rd Party Packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2\n",
    "from sigopt import Connection\n",
    "import pycm\n",
    "# Initialize the optimizer and model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.models import save_model\n",
    "# User Packages\n",
    "from start.preprocessing import ImageToTensorPreprocessor, ResizePreprocessor, ColorSpacePreprocessor\n",
    "from start.loader import ImageCachedDataset\n",
    "from start.model import MobileNetV2\n",
    "from keys.sigopt import token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 224\n",
    "HEIGHT = WIDTH\n",
    "CHANNELS = 1\n",
    "BATCH_SIZE = 32\n",
    "# Initialize optimizer\n",
    "N_EPOCHS = 100\n",
    "learning_rate = 0.01\n",
    "decay_rate = learning_rate / N_EPOCHS\n",
    "N_TRAINABLE_LAYERS = 152\n",
    "timestamp = time.time()\n",
    "OUTPUT_DIR = r'/home/share/education/deep_learning/pyimagesearch/models/flower-17-sigopt/'\n",
    "EXPERIMENT_ID = 57907\n",
    "nLoops = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing label: dandelion\n[INFO] processed 80 dandelion images\n[INFO] Processing label: coltsfoot\n[INFO] processed 80 coltsfoot images\n[INFO] Processing label: lilyvalley\n[INFO] processed 80 lilyvalley images\n[INFO] Processing label: cowslip\n[INFO] processed 80 cowslip images\n[INFO] Processing label: iris\n[INFO] processed 80 iris images\n[INFO] Processing label: snowdrop\n[INFO] processed 80 snowdrop images\n[INFO] Processing label: sunflower\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processed 80 sunflower images\n[INFO] Processing label: tigerlily\n[INFO] processed 80 tigerlily images\n[INFO] Processing label: daisy\n[INFO] processed 80 daisy images\n[INFO] Processing label: pansy\n[INFO] processed 80 pansy images\n[INFO] Processing label: crocus\n[INFO] processed 80 crocus images\n[INFO] Processing label: daffodil\n[INFO] processed 80 daffodil images\n[INFO] Processing label: fritillary\n[INFO] processed 80 fritillary images\n[INFO] Processing label: buttercup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processed 80 buttercup images\n[INFO] Processing label: windflower\n[INFO] processed 80 windflower images\n[INFO] Processing label: files.txt\n[INFO] Processing label: tulip\n[INFO] processed 80 tulip images\n[INFO] Processing label: bluebell\n[INFO] processed 80 bluebell images\ndata shape: (1360, 224, 224, 3)\nlabels shape: (1360,)\n"
     ]
    }
   ],
   "source": [
    "# Load Flowers-17 dataset\n",
    "dataset = ImageCachedDataset(\n",
    "    preprocessors=[\n",
    "        ResizePreprocessor(width=WIDTH, height=HEIGHT, aspect_preserving=True),\n",
    "        ColorSpacePreprocessor(conversion=cv2.COLOR_BGR2GRAY),\n",
    "        ImageToTensorPreprocessor()\n",
    "    ],\n",
    "    dataset_path=r'/home/share/dataset/flowers17'\n",
    ")\n",
    "(data, labels) = dataset.load(\n",
    "    verbosity=80\n",
    ")\n",
    "\n",
    "print('data shape: {}'.format(data.shape))\n",
    "print('labels shape: {}'.format(labels.shape))\n",
    "\n",
    "classes = set(labels)\n",
    "\n",
    "# Normalize data\n",
    "data = data.astype(np.float) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data splits\n",
    "# Partition into train and test splits\n",
    "(trainX, testX, trainY, testY) = train_test_split(\n",
    "    data, labels,\n",
    "    test_size=0.2,\n",
    "    random_state=int(time.time()),\n",
    "    stratify=list(labels)\n",
    ")\n",
    "(valX, testX, valY, testY) = train_test_split(\n",
    "    testX, testY,\n",
    "    test_size=0.4,\n",
    "    random_state=int(time.time()),\n",
    "    stratify=list(testY)\n",
    ")\n",
    "\n",
    "# Binarize output to one hot vectors\n",
    "lb = LabelBinarizer()\n",
    "trainY = lb.fit_transform(trainY)\n",
    "valY = lb.fit_transform(valY)\n",
    "testY = lb.fit_transform(testY)\n",
    "\n",
    "# Data augmentation\n",
    "augmenter = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created experiment: https://app.sigopt.com/experiment/57907\n"
     ]
    }
   ],
   "source": [
    "# Sigopt\n",
    "sigopt_connection = Connection(client_token=token)\n",
    "\"\"\"\n",
    "sigopt_experiment = sigopt_connection.experiments().create(\n",
    "    name='Flowers-17-MobileNetV2-{}'.format(timestamp),\n",
    "    parameters=[\n",
    "        dict(name='dense_units', type='int', bounds=dict(min=128, max=512)),\n",
    "        dict(name='dropout_rate', type='double', bounds=dict(min=0.2, max=0.6)),\n",
    "        dict(name='regularization_strength', type='double', bounds=dict(min=0.0001, max=0.01))\n",
    "    ],\n",
    "    metrics=[dict(name='function_value')],\n",
    "    parallel_bandwidth=1,\n",
    "    observation_budget=30\n",
    ")\n",
    "\"\"\"\n",
    "sigopt_experiment = sigopt_connection.experiments(EXPERIMENT_ID).fetch()\n",
    "print(\"Created experiment: https://app.sigopt.com/experiment/\" + sigopt_experiment.id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ImageNet weights w/ Non-Standard Input Dimensions, redefining MobileNetV2 head...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training network...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected mobilenetv2_1.40_224_input to have shape (224, 224, 1) but got array with shape (224, 224, 3)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f76065c2cfb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     )\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/start-learning/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2175\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2176\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   2178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/PycharmProjects/start-learning/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    102\u001b[0m             'or `(val_x, val_y)`. Found: ' + str(validation_data))\n\u001b[1;32m    103\u001b[0m       val_x, val_y, val_sample_weights = model._standardize_user_data(\n\u001b[0;32m--> 104\u001b[0;31m           val_x, val_y, val_sample_weights)\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     callbacks = cbks.configure_callbacks(\n",
      "\u001b[0;32m~/PycharmProjects/start-learning/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_element\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     x, y, sample_weights = self._standardize_weights(x, y, sample_weight,\n\u001b[0;32m--> 992\u001b[0;31m                                                      class_weight, batch_size)\n\u001b[0m\u001b[1;32m    993\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/start-learning/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_weights\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m         exception_prefix='input')\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/start-learning/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0;34m'Error when checking '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexception_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                 \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m                 ' but got array with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    333\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected mobilenetv2_1.40_224_input to have shape (224, 224, 1) but got array with shape (224, 224, 3)"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Run the optimization loop until observation budget is exhausted or local budget exhausted\n",
    "while sigopt_experiment.progress.observation_count < sigopt_experiment.observation_budget or nLoops > 0:\n",
    "    # Get SigOpt suggestions\n",
    "    try:\n",
    "        suggestion = sigopt_connection.experiments(sigopt_experiment.id).suggestions().create()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Build model \n",
    "    model = MobileNetV2.build({\n",
    "        'width':        WIDTH,\n",
    "        'height':       HEIGHT,\n",
    "        'channels':     CHANNELS,\n",
    "        'classes':      classes,\n",
    "        'weights':      'imagenet',\n",
    "        'dense_units':  242,\n",
    "        'dropout_rate': 0.43,\n",
    "        'regularization_strength': 0.0001,\n",
    "    })\n",
    "    \n",
    "    \"\"\"\n",
    "    tb_callback = TensorBoard(\n",
    "        log_dir='./logs/{}'.format(timestamp), \n",
    "        histogram_freq=2, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        write_graph=False, \n",
    "        write_grads=False, \n",
    "        write_images=False, \n",
    "        embeddings_freq=0,\n",
    "        embeddings_layer_names=None, \n",
    "        embeddings_metadata=None, \n",
    "        embeddings_data=None\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    opt = SGD(\n",
    "        lr=learning_rate,\n",
    "        momentum=0,\n",
    "        decay=0,\n",
    "        nesterov=False\n",
    "    )\n",
    "    \"\"\"\n",
    "    opt = Adam(\n",
    "        lr=learning_rate,\n",
    "        beta_1=0.99,\n",
    "        beta_2=0.999,\n",
    "        epsilon=0.1,\n",
    "        decay=decay_rate,\n",
    "        amsgrad=False\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=opt,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Train the network\n",
    "    print('[INFO] training network...')\n",
    "    history = model.fit_generator(\n",
    "        augmenter.flow(trainX, trainY, \n",
    "                       batch_size=BATCH_SIZE),\n",
    "        validation_data=(valX, valY),\n",
    "        steps_per_epoch=len(trainX) // BATCH_SIZE,\n",
    "        epochs=N_EPOCHS,\n",
    "        callbacks=[],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate the network\n",
    "    print('[INFO] evaluating network...')\n",
    "    predictions = model.predict(testX, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    cm = pycm.ConfusionMatrix(\n",
    "        actual_vector=lb.inverse_transform(testY),\n",
    "        predict_vector=lb.inverse_transform(predictions)\n",
    "    )\n",
    "    \n",
    "    model_string = ('{0}_flowers-17-bw-valacc{1:.3f}-valloss{2:.3f}-du{3}-do{4:.3f}-rs{5:.3f}'.format(\n",
    "        timestamp, \n",
    "        history.history['val_acc'][-1],\n",
    "        history.history['val_loss'][-1],\n",
    "        242,\n",
    "        0.43,\n",
    "        0.0001\n",
    "    )).replace('.', ',')\n",
    "    # Save CM\n",
    "    cm.save_html(os.path.join(OUTPUT_DIR, '{}_confusion_matrix'.format(model_string)))\n",
    "    # Save model\n",
    "    save_model(\n",
    "        model,\n",
    "        os.path.join(OUTPUT_DIR, model_string + '.h5')\n",
    "    )\n",
    "    \n",
    "    # Plot the training loss and accuracy\n",
    "    plt.style.use('ggplot')\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(0, N_EPOCHS), history.history['loss'], label='train_loss')\n",
    "    plt.plot(np.arange(0, N_EPOCHS), history.history['val_loss'], label='val_loss')\n",
    "    plt.plot(np.arange(0, N_EPOCHS), history.history['acc'], label='train_acc')\n",
    "    plt.plot(np.arange(0, N_EPOCHS), history.history['val_acc'], label='val_acc')\n",
    "    plt.title('Training Loss and Accuracy')\n",
    "    plt.xlabel('Epoch #')\n",
    "    plt.ylabel('Loss/Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, model_string+'.png'))\n",
    "    \n",
    "    try:\n",
    "        # Report to SigOpt\n",
    "        sigopt_connection.experiments(sigopt_experiment.id).observations().create(\n",
    "            suggestion=suggestion.id,\n",
    "            value=1.0/(history.history['val_loss'][-1])\n",
    "        )\n",
    "        # Update the experiment object\n",
    "        sigopt_experiment = sigopt_connection.experiments(sigopt_experiment.id).fetch()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    nLoops -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the best configuration and explore your experiment\n",
    "best_assignments = sigopt_connection.experiments(sigopt_experiment.id).best_assignments().fetch().data[0].assignments\n",
    "print(\"Best Assignments: \" + best_assignments)\n",
    "print(\"Explore your experiment: https://app.sigopt.com/experiment/\" + sigopt_experiment.id + \"/analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here the most common ways to prevent overfitting in neural networks:\n",
    "\n",
    "* Get more training data.\n",
    "* Reduce the capacity of the network.\n",
    "* Add weight regularization.\n",
    "* Add dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
