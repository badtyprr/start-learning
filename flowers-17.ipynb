{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fine-grained classification practice with Flower-17\n",
    "\"\"\"\n",
    "\n",
    "# Python Packages\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "# 3rd Party Packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "import cv2\n",
    "# User Packages\n",
    "from start.preprocessing import ImageToTensorPreprocessor, ResizePreprocessor, ColorSpacePreprocessor\n",
    "from start.loader import ImageDataset\n",
    "from start.model import MiniVGGNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Flowers-17 dataset\n",
    "dataset = ImageDataset(\n",
    "    preprocessors=[\n",
    "        ResizePreprocessor(224, 224, aspect_preserving=True),\n",
    "        ColorSpacePreprocessor(conversion=cv2.COLOR_BGR2GRAY),\n",
    "        ImageToTensorPreprocessor()\n",
    "    ]\n",
    ")\n",
    "(data, labels) = dataset.load(\n",
    "    dataset_path=r'/home/share/dataset/flowers17',\n",
    "    verbosity=80\n",
    ")\n",
    "\n",
    "print('data shape: {}'.format(data.shape))\n",
    "print('labels shape: {}'.format(labels.shape))\n",
    "\n",
    "classes = set(labels)\n",
    "\n",
    "# Normalize data\n",
    "data = data.astype(np.float) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data splits\n",
    "# Partition into train and test splits\n",
    "(trainX, testX, trainY, testY) = train_test_split(\n",
    "    data, labels,\n",
    "    test_size=0.2,\n",
    "    random_state=int(time.time()),\n",
    "    stratify=list(labels)\n",
    ")\n",
    "(valX, testX, valY, testY) = train_test_split(\n",
    "    testX, testY,\n",
    "    test_size=0.4,\n",
    "    random_state=int(time.time()),\n",
    "    stratify=list(testY)\n",
    ")\n",
    "\n",
    "# Binarize output to one hot vectors\n",
    "lb = LabelBinarizer()\n",
    "trainY = lb.fit_transform(trainY)\n",
    "valY = lb.fit_transform(valY)\n",
    "testY = lb.fit_transform(testY)\n",
    "\n",
    "# Data augmentation\n",
    "augmenter = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the optimizer and model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import pycm\n",
    "from tensorflow.keras.models import save_model\n",
    "import time\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "# Initialize optimizer\n",
    "N_EPOCHS = 100\n",
    "learning_rate = 0.01\n",
    "decay_rate = learning_rate / N_EPOCHS\n",
    "N_TRAINABLE_LAYERS = 152\n",
    "timestamp = time.time()\n",
    "OUTPUT_DIR = r'/home/share/education/deep_learning/pyimagesearch/models/flower-17-bw/'\n",
    "\n",
    "for trainable_layer in list(range(-N_TRAINABLE_LAYERS, 0))[::-1]:\n",
    "    clear_session()\n",
    "    print('[INFO] compiling model to be trainable until layer {}...'.format(N_TRAINABLE_LAYERS+trainable_layer+1))\n",
    "    properties = {\n",
    "        'width':    224,\n",
    "        'height':   224,\n",
    "        'channels': 1,\n",
    "        'classes':  len(classes)\n",
    "    }\n",
    "    #model = MiniVGGNet.build(properties)\n",
    "    \n",
    "    mnv2_imagenet = MobileNetV2(\n",
    "        input_shape=(224, 224, 3),\n",
    "        alpha=1.4,\n",
    "        depth_multiplier=1, \n",
    "        include_top=False, \n",
    "        weights='imagenet', \n",
    "        input_tensor=None, \n",
    "        pooling='avg'\n",
    "    )\n",
    "    mnv2 = MobileNetV2(\n",
    "        input_shape=(224, 224, 1),\n",
    "        alpha=1.4,\n",
    "        depth_multiplier=1, \n",
    "        include_top=False, \n",
    "        weights=None, \n",
    "        input_tensor=None, \n",
    "        pooling='avg'\n",
    "    )\n",
    "    \n",
    "    for i, layer in enumerate(mnv2_imagenet.layers[3:152]):\n",
    "        weights = mnv2_imagenet.get_layer(name=layer.get_config()['name']).get_weights()\n",
    "        mnv2.get_layer(name=layer.get_config()['name']).set_weights(weights)\n",
    "    \n",
    "    layer = mnv2_imagenet.layers[2]\n",
    "    weights = mnv2_imagenet.get_layer(name=layer.get_config()['name']).get_weights()\n",
    "    weights[0] = np.reshape(np.mean(weights[0], axis=2), (3, 3, 1, 48))\n",
    "    mnv2.get_layer(name=layer.get_config()['name']).set_weights(weights)\n",
    "    \n",
    "    for layer in mnv2.layers[:trainable_layer]:\n",
    "        layer.trainable = False\n",
    "    trainable_layer_name = mnv2.layers[trainable_layer].get_config()['name']\n",
    "            \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(mnv2)\n",
    "    # MobileNetV2 uses a GAP layer which essentially \"Flattens\" the feature maps already, so no call to Flatten here\n",
    "    model.add(Dense(\n",
    "        units=256,\n",
    "        activation='relu',\n",
    "        use_bias=True,\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        bias_initializer='glorot_uniform',\n",
    "        kernel_regularizer=l2(0.001),\n",
    "        bias_regularizer=l2(0.001),\n",
    "        activity_regularizer=None,\n",
    "        kernel_constraint=None,\n",
    "        bias_constraint=None\n",
    "    ))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(\n",
    "        units=len(classes),\n",
    "        activation='softmax',\n",
    "        use_bias=True,\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        bias_initializer='glorot_uniform',\n",
    "        kernel_regularizer=l2(0.001),\n",
    "        bias_regularizer=l2(0.001),\n",
    "        activity_regularizer=None,\n",
    "        kernel_constraint=None,\n",
    "        bias_constraint=None\n",
    "    ))\n",
    "    \n",
    "    \"\"\"\n",
    "    tb_callback = TensorBoard(\n",
    "        log_dir='./logs/{}'.format(timestamp), \n",
    "        histogram_freq=2, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        write_graph=False, \n",
    "        write_grads=False, \n",
    "        write_images=False, \n",
    "        embeddings_freq=0,\n",
    "        embeddings_layer_names=None, \n",
    "        embeddings_metadata=None, \n",
    "        embeddings_data=None\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    opt = SGD(\n",
    "        lr=learning_rate,\n",
    "        momentum=0,\n",
    "        decay=0,\n",
    "        nesterov=False\n",
    "    )\n",
    "    \"\"\"\n",
    "    opt = Adam(\n",
    "        lr=learning_rate,\n",
    "        beta_1=0.99,\n",
    "        beta_2=0.999,\n",
    "        epsilon=0.1,\n",
    "        decay=decay_rate,\n",
    "        amsgrad=False\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=opt,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Train the network\n",
    "    print('[INFO] training network...')\n",
    "    history = model.fit_generator(\n",
    "        augmenter.flow(trainX, trainY, \n",
    "                       batch_size=BATCH_SIZE),\n",
    "        validation_data=(valX, valY),\n",
    "        steps_per_epoch=len(trainX) // BATCH_SIZE,\n",
    "        epochs=N_EPOCHS,\n",
    "        callbacks=[],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate the network\n",
    "    print('[INFO] evaluating network...')\n",
    "    predictions = model.predict(testX, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    cm = pycm.ConfusionMatrix(\n",
    "        actual_vector=lb.inverse_transform(testY),\n",
    "        predict_vector=lb.inverse_transform(predictions)\n",
    "    )\n",
    "    cm.save_html(os.path.join(OUTPUT_DIR, '{}_confusion_matrix_layer_{}.html'.format(timestamp, trainable_layer_name)))\n",
    "    \n",
    "    model_string = ('{0}_flowers-17-bw-valacc{1:.3f}-valloss{2:.3f}_layer_{3}'.format(\n",
    "        timestamp, \n",
    "        history.history['val_acc'][-1], history.history['val_loss'][-1],\n",
    "        trainable_layer_name\n",
    "    )).replace('.', ',')\n",
    "    # Save model\n",
    "    save_model(\n",
    "        model,\n",
    "        os.path.join(OUTPUT_DIR, model_string + '.h5')\n",
    "    )\n",
    "    \n",
    "    # Plot the training loss and accuracy\n",
    "    plt.style.use('ggplot')\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(0, N_EPOCHS), history.history['loss'], label='train_loss')\n",
    "    plt.plot(np.arange(0, N_EPOCHS), history.history['val_loss'], label='val_loss')\n",
    "    plt.plot(np.arange(0, N_EPOCHS), history.history['acc'], label='train_acc')\n",
    "    plt.plot(np.arange(0, N_EPOCHS), history.history['val_acc'], label='val_acc')\n",
    "    plt.title('Training Loss and Accuracy')\n",
    "    plt.xlabel('Epoch #')\n",
    "    plt.ylabel('Loss/Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, model_string+'.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here the most common ways to prevent overfitting in neural networks:\n",
    "\n",
    "* Get more training data.\n",
    "* Reduce the capacity of the network.\n",
    "* Add weight regularization.\n",
    "* Add dropout."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
